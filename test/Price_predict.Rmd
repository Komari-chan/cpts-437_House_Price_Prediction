---
title: "House Prediction"
author: "Team"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
# Load libraries
library(tidyverse)
library(caret)
library(gbm)
library(xgboost)
library(randomForest)
library(ggplot2)

# Load data
housing_data <- read.csv("C:/Users/skyli/Desktop/437/CPTS_437_data/whitman_property_details.csv")

# Data cleaning and feature engineering
clean_data <- housing_data %>%
  mutate(
    Total_Area = as.numeric(gsub(",", "", ifelse(Total_Area == "None", NA, Total_Area))),
    Year_Built = as.numeric(ifelse(Year_Built == "None", NA, Year_Built)),
    Total_Value = as.numeric(gsub(",", "", ifelse(Total_Value == "None", 0, Total_Value))),
    Bedrooms = as.numeric(ifelse(Bedrooms == "None", NA, Bedrooms)),
    Bathrooms = as.numeric(ifelse(Bathrooms == "None", NA, Bathrooms)),
    Garage_Stalls = as.numeric(ifelse(Garage_Stalls %in% c("None", "Block"), 0, Garage_Stalls))
  ) %>%
   filter(!is.na(Total_Area)) %>%  # Exclude rows with Total_Area as NA
  filter(!is.na(Total_Value) & !is.na(Year_Built)) %>%
  mutate(
    log_value = log(Total_Value + 1),
    log_area = log(Total_Area + 1),
    age = 2024 - Year_Built,
    has_garage = ifelse(is.na(Garage_Stalls), 0, 1),
    rooms_per_area = (Bedrooms + Bathrooms) / log_area,
    condition_score = case_when(
      grepl("3.0", Condition) ~ 3.0,
      grepl("3.5", Condition) ~ 3.5,
      grepl("4.0", Condition) ~ 4.0,
      TRUE ~ 3.0
    ),
    age_condition_interaction = age * condition_score
  ) %>%
  filter(
    Total_Value > quantile(Total_Value, 0.03) & Total_Value < quantile(Total_Value, 0.97),
    Total_Area > quantile(Total_Area, 0.03) & Total_Area < quantile(Total_Area, 0.97)
  ) %>%
  select(log_value, log_area, age, has_garage, rooms_per_area, Bathrooms, 
         condition_score, age_condition_interaction) %>%
  na.omit()

# Split data
set.seed(100)
train_index <- createDataPartition(clean_data$log_value, p = 0.8, list = FALSE)
train_data <- clean_data[train_index, ]
test_data <- clean_data[-train_index, ]

# GBM Model
gbm_model <- gbm(
  log_value ~ .,
  data = train_data,
  distribution = "gaussian",
  n.trees = 3000,
  interaction.depth = 8,
  shrinkage = 0.005,
  n.minobsinnode = 8,
  bag.fraction = 0.8,
  cv.folds = 5
)
best_iter_gbm <- gbm.perf(gbm_model, method = "cv")

# XGBoost Model
train_matrix <- xgb.DMatrix(data = as.matrix(train_data %>% select(-log_value)), label = train_data$log_value)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data %>% select(-log_value)))
xgb_model <- xgboost(
  data = train_matrix,
  objective = "reg:squarederror",
  nrounds = 2000,
  max_depth = 6,
  eta = 0.01,
  subsample = 0.8,
  colsample_bytree = 0.8,
  verbose = 0
)

# Random Forest Model
rf_model <- randomForest(
  log_value ~ .,
  data = train_data,
  ntree = 500,
  mtry = floor(sqrt(ncol(train_data))),
  importance = TRUE
)

# Predictions and Metrics
gbm_predictions <- exp(predict(gbm_model, test_data, n.trees = best_iter_gbm)) - 1
xgb_predictions <- exp(predict(xgb_model, test_matrix)) - 1
rf_predictions <- exp(predict(rf_model, test_data)) - 1
actual_values <- exp(test_data$log_value) - 1

metrics <- function(predictions, actual) {
  rmse <- sqrt(mean((predictions - actual)^2))
  r2 <- 1 - sum((actual - predictions)^2) / sum((actual - mean(actual))^2)
  mae <- mean(abs(predictions - actual))
  return(list(RMSE = rmse, R2 = r2, MAE = mae))
}

gbm_metrics <- metrics(gbm_predictions, actual_values)
xgb_metrics <- metrics(xgb_predictions, actual_values)
rf_metrics <- metrics(rf_predictions, actual_values)

# Print metrics
cat("\nGBM Metrics:\n")
print(gbm_metrics)

cat("\nXGBoost Metrics:\n")
print(xgb_metrics)

cat("\nRandom Forest Metrics:\n")
print(rf_metrics)

# Visualization
importance_gbm <- summary(gbm_model, n.trees = best_iter_gbm)
importance_xgb <- xgb.importance(model = xgb_model)

# Feature importance plots
ggplot(data.frame(Feature = importance_gbm$var, Importance = importance_gbm$rel.inf), aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance (GBM)", x = "Features", y = "Importance")

xgb.plot.importance(importance_xgb)

# Residual plots
gbm_residuals <- actual_values - gbm_predictions
xgb_residuals <- actual_values - xgb_predictions
rf_residuals <- actual_values - rf_predictions

# GBM Residual Plot
ggplot(data.frame(fitted = gbm_predictions, residuals = gbm_residuals), aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residual Plot (GBM)", x = "Fitted Values", y = "Residuals")

# XGBoost Residual Plot
ggplot(data.frame(fitted = xgb_predictions, residuals = xgb_residuals), aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residual Plot (XGBoost)", x = "Fitted Values", y = "Residuals")

# Random Forest Residual Plot
ggplot(data.frame(fitted = rf_predictions, residuals = rf_residuals), aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residual Plot (Random Forest)", x = "Fitted Values", y = "Residuals")
```
